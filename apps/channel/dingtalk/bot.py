import json
import logging
import re

import httpx
from bs4 import BeautifulSoup
from dingtalk_stream import AckMessage, ChatbotHandler

from apps.channel.models import ChannelUser, Message
from apps.ai.classifier import classify_message, classify_article
from apps.ai.extractor import extract_task
from apps.ai.responder import generate_reply
from apps.todo.models import Task
from apps.todo.notion_client import save_link_to_knowledge_base

logger = logging.getLogger(__name__)


class YgaiBotHandler(ChatbotHandler):
    """å¤„ç†é’‰é’‰æœºå™¨äººæ”¶åˆ°çš„æ¶ˆæ¯"""

    async def process(self, callback):
        try:
            from asgiref.sync import sync_to_async

            incoming = callback.data
            logger.info("æ”¶åˆ°åŸå§‹æ¶ˆæ¯: %s", incoming)

            # å¤„ç†å¯Œæ–‡æœ¬/å›¾æ–‡/èŠå¤©è®°å½•è½¬å‘æ¶ˆæ¯
            msgtype = incoming.get('msgtype', '')
            text = ''
            download_codes = []

            if msgtype == 'richText':
                rich_text = incoming.get('content', {}).get('richText', [])
                for item in rich_text:
                    if 'text' in item:
                        text += item['text']
                    elif 'downloadCode' in item:
                        download_codes.append(item['downloadCode'])
            elif msgtype == 'chatRecord':
                # è§£æè½¬å‘çš„èŠå¤©è®°å½•
                try:
                    chat_records = json.loads(incoming.get('content', {}).get('chatRecord', '[]'))
                    record_texts = []
                    for record in chat_records:
                        record_msg_type = record.get('msgType', '')
                        if record_msg_type == 'text':
                            record_texts.append(record.get('content', ''))
                        elif record_msg_type == 'richText':
                            for item in record.get('richText', []):
                                if item.get('msgType') == 'text':
                                    record_texts.append(item.get('content', ''))
                                elif item.get('msgType') == 'picture' and 'downloadCode' in item:
                                    download_codes.append(item['downloadCode'])

                    text = "\n".join(record_texts)
                except json.JSONDecodeError:
                    text = incoming.get('content', {}).get('summary', '')
            elif msgtype == 'picture':
                # å•çº¯çš„ä¸€å¼ å›¾ç‰‡
                code = incoming.get('content', {}).get('downloadCode', '')
                if code:
                    download_codes.append(code)
                text = "è¿™æ˜¯å›¾ç‰‡æ¶ˆæ¯ï¼Œè¯·æå–å›¾ç‰‡ä¸­çš„ä»»åŠ¡ä¿¡æ¯" # ç»™ AI è¡¥å……ä¸€ä¸ªé»˜è®¤æ–‡æœ¬æç¤º

                # å¦‚æœæ˜¯çº¯å›¾ç‰‡æ¶ˆæ¯ï¼Œæˆ‘ä»¬åœ¨æ•°æ®åº“é‡ŒæŠŠ message_type è®¾ä¸º image
                msg_record_type = 'image'
            else:
                text = incoming.get('text', {}).get('content', '')
                msg_record_type = 'text'

            # åªæœ‰åœ¨è¿˜æ²¡è¢«è®¾ä¸º image çš„æƒ…å†µä¸‹ï¼Œå¦‚æœæ˜¯æ–‡æœ¬/å›¾æ–‡ï¼Œè®¾ä¸º text
            if 'msg_record_type' not in locals():
                msg_record_type = 'text'

            text = text.strip()
            sender_id = incoming.get('senderStaffId', '')
            sender_nick = incoming.get('senderNick', '')
            msg_id = incoming.get('msgId', '')
            conversation_id = incoming.get('conversationId', '')

            if not text and not download_codes:
                self.reply_text('è¯·å‘é€æ–‡æœ¬æˆ–å›¾ç‰‡æ¶ˆæ¯', callback)
                return AckMessage.STATUS_OK, 'OK'

            logger.info("æ”¶åˆ°æ¶ˆæ¯ from %s: %s (é™„å¸¦ %d å¼ å›¾ç‰‡)", sender_nick, text, len(download_codes))

            # 1. å¦‚æœæ˜¯ç¾¤èŠä¸­@æœºå™¨äººï¼Œé’‰é’‰ä¼šå°†@æœºå™¨äººçš„æ–‡æœ¬ä¹Ÿå¸¦è¿‡æ¥ï¼ˆå¦‚ "@YGAI å¸®æˆ‘å†™ä¸ªè¯·å‡æ¡"ï¼‰
            clean_text = text
            if incoming.get('conversationType') == '2':  # å¦‚æœæ˜¯ç¾¤èŠ
                at_users = incoming.get('atUsers', [])
                for at_user in at_users:
                    at_dingtalk_id = at_user.get('dingtalkId')
                    clean_text = clean_text.replace(f'@{at_dingtalk_id}', '').strip()

            # URL æå–ä¸ä¿¡æ¯è·å–
            urls = re.findall(r'https?://[^\s\u4e00-\u9fff<>"\'\n\r]+', clean_text)
            url_infos = []
            if urls:
                async with httpx.AsyncClient(timeout=10.0, follow_redirects=True, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}) as client:
                    for url in urls:
                        # æ£€æŸ¥é“¾æ¥æ˜¯å¦å·²ç»å­˜åœ¨
                        from apps.todo.notion_client import check_link_exists_in_knowledge_base
                        existing_info = await sync_to_async(check_link_exists_in_knowledge_base)(url)
                        if existing_info and existing_info.get("exists"):
                            logger.info("URL å·²å­˜åœ¨äºçŸ¥è¯†åº“ï¼Œè·³è¿‡æŠ“å–ä¸ä¿å­˜: %s", url)
                            url_infos.append({
                                "url": url,
                                "title": existing_info["title"],
                                "category": existing_info["category"],
                                "rating": existing_info["rating"],
                                "summary": existing_info["summary"],
                                "is_existing": True
                            })
                            continue

                        title = url
                        publish_date = None
                        try:
                            response = await client.get(url)
                            if response.status_code == 200:
                                soup = BeautifulSoup(response.text, 'html.parser')

                                # 1. å°è¯•è·å–æ ‡é¢˜
                                # å¾®ä¿¡æ–‡ç« çš„çœŸå®æ ‡é¢˜ä¸€èˆ¬åœ¨ meta æ ‡ç­¾ä¸­ï¼Œä¼˜å…ˆå– og:title
                                og_title = soup.find('meta', property='og:title')
                                if og_title and og_title.get('content'):
                                    title = og_title['content'].strip()
                                elif soup.title and soup.title.string:
                                    title = soup.title.string.strip()

                                # 2. å°è¯•è·å–æ–‡ç« å‘å¸ƒæ—¶é—´
                                # ä» Open Graph è·å–
                                og_time = soup.find('meta', property='article:published_time') or soup.find('meta', property='og:article:published_time')
                                if og_time and og_time.get('content'):
                                    publish_date = og_time['content'].strip()
                                else:
                                    # ä»å¸¸è§çš„ meta name è·å–
                                    meta_time = soup.find('meta', attrs={'name': 'publishdate'}) or soup.find('meta', attrs={'name': 'pubdate'})
                                    if meta_time and meta_time.get('content'):
                                        publish_date = meta_time['content'].strip()
                                    else:
                                        # ç‰¹æ®Šå¤„ç†å¾®ä¿¡æ–‡ç« çš„å‘å¸ƒæ—¶é—´ (å¾®ä¿¡é¡µé¢ä¸­é€šå¸¸æœ‰ä¸€ä¸ªç‰¹å®šå±æ€§æˆ–æ³¨é‡Šï¼Œè¿™é‡Œå°è¯•è·å– js å˜é‡)
                                        time_match = re.search(r'create_time\s*=\s*"([^"]+)"', response.text) or re.search(r'ct\s*=\s*"(\d{10})"', response.text)
                                        if time_match:
                                            time_val = time_match.group(1)
                                            if time_val.isdigit() and len(time_val) == 10:
                                                from datetime import datetime, timezone, timedelta
                                                # å¾®ä¿¡çš„ Unix æ—¶é—´æˆ³å·²ç»æ˜¯ UTC æ—¶é—´ï¼Œå°†å…¶è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´ (UTC+8) çš„å¸¦æ—¶åŒºæ ¼å¼
                                                # å…ˆè·å– UTC çš„ datetime å¯¹è±¡
                                                dt_utc = datetime.fromtimestamp(int(time_val), tz=timezone.utc)
                                                # å†è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´çš„ datetime å¯¹è±¡
                                                dt_beijing = dt_utc.astimezone(timezone(timedelta(hours=8)))
                                                publish_date = dt_beijing.isoformat()
                                            else:
                                                publish_date = time_val
                                        else:
                                            # å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•æ—¶é—´ï¼Œå¯¹äºéå¾®ä¿¡çš„æ–‡ç« ä¹Ÿå°è¯•ç”¨æ­£åˆ™æŠ“å–ç±»ä¼¼ "2024-02-27" è¿™æ ·çš„æ—¥æœŸ
                                            date_match = re.search(r'\b(20[12]\d[-/å¹´](0?[1-9]|1[012])[-/æœˆ](0?[1-9]|[12][0-9]|3[01])[æ—¥]?)\b', response.text)
                                            if date_match:
                                                import datetime as dt_lib
                                                raw_date = date_match.group(1).replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace('/', '-')
                                                try:
                                                    # å°è¯•è½¬æ¢ä¸ºæ ‡å‡†çš„å¸¦æ—¶åŒº ISO æ ¼å¼
                                                    parsed_date = dt_lib.datetime.strptime(raw_date, "%Y-%m-%d")
                                                    publish_date = parsed_date.replace(tzinfo=dt_lib.timezone(dt_lib.timedelta(hours=8))).isoformat()
                                                except ValueError:
                                                    publish_date = raw_date

                        except Exception as e:
                            logger.warning("è·å– URL æ ‡é¢˜æˆ–æ—¶é—´å¤±è´¥ %s: %s", url, e)

                        category = await sync_to_async(classify_article)(title)

                        # ä½¿ç”¨ AI è¿›è¡Œæ·±åº¦åˆ†æï¼šæå–çœŸå®æ¥æºã€è¯„åˆ†ã€æ¦‚è¦
                        from apps.ai.classifier import analyze_article_content
                        content_text = soup.get_text(separator='\n', strip=True) if 'soup' in locals() else ""
                        analysis_result = await sync_to_async(analyze_article_content)(title, url, content_text)

                        source = analysis_result.get("source", sender_nick)
                        if source == "æœªçŸ¥æ¥æº":
                            source = sender_nick

                        rating = analysis_result.get("rating", "â­â­â­")
                        summary = analysis_result.get("summary", "æš‚æ— æ‘˜è¦")

                        logger.info("AI ç½‘é¡µåˆ†æç»“æœ - URL: %s | æ ‡é¢˜: %s | åˆ†ç±»: %s | å‘å¸ƒæ—¶é—´: %s | æ¥æº: %s | è¯„åˆ†: %s",
                                    url, title, category, publish_date, source, rating)

                        try:
                            await sync_to_async(save_link_to_knowledge_base)(url, title, source, category, publish_date, rating, summary)
                            url_infos.append({
                                "url": url,
                                "title": title,
                                "category": category,
                                "rating": rating,
                                "summary": summary
                            })
                        except Exception as e:
                            logger.error("ä¿å­˜ URL åˆ° Notion å¤±è´¥ %s: %s", url, e)

            # 2. è¯†åˆ«/åˆ›å»ºæ¸ é“ç”¨æˆ·
            channel_user, _ = await sync_to_async(ChannelUser.objects.get_or_create)(
                platform='dingtalk',
                platform_user_id=sender_id,
                defaults={'name': sender_nick},
            )

            # 3. å‡†å¤‡å›¾ç‰‡ URLs (å¤šå¼ å›¾ç‰‡)
            image_urls = []
            if download_codes:
                from .utils import get_download_url
                for code in download_codes:
                    url = await get_download_url(code, incoming.get('robotCode', ''))
                    if url:
                        image_urls.append(url)

            # 4. ä¿å­˜åŸå§‹æ¶ˆæ¯
            # å¦‚æœæ˜¯å¤šå¼ å›¾ç‰‡ï¼Œæˆ‘ä»¬å°† URLs ç”¨é€—å·è¿æ¥å­˜å…¥æ•°æ®åº“
            saved_content = text
            if msgtype == 'picture':
                saved_content = ",".join(image_urls) if image_urls else text

            message = await sync_to_async(Message.objects.create)(
                channel_user=channel_user,
                platform='dingtalk',
                content=saved_content,
                message_type=msg_record_type,
                direction='inbound',
                platform_message_id=msg_id,
            )

            # 5. é€å¼ è¯†åˆ«å›¾ç‰‡å†…å®¹
            image_descriptions = []
            if image_urls:
                from apps.ai.recognizer import recognize_images
                image_descriptions = await sync_to_async(recognize_images)(image_urls)
                logger.info("å›¾ç‰‡è¯†åˆ«ç»“æœ: %s", image_descriptions)

            # 6. AI åˆ†ç±» (å°†å›¾ç‰‡è¯†åˆ«æ–‡æœ¬æ‹¼å…¥ï¼Œè®©åˆ†ç±»æ›´å‡†ç¡®)
            if image_descriptions:
                full_text = clean_text + "\n\n" + "\n".join(image_descriptions) if clean_text else "\n".join(image_descriptions)
            else:
                full_text = clean_text

            if not full_text:
                classification = 'important'
            else:
                classification = await sync_to_async(classify_message)(full_text)

            message.ai_classification = classification
            message.processed = True
            await sync_to_async(message.save)(update_fields=['ai_classification', 'processed'])
            logger.info("AI åˆ†ç±»ç»“æœ: %s", classification)

            is_group = incoming.get('conversationType') == '2'

            # 7. æ ¹æ®åˆ†ç±»å¤„ç†
            if classification in ('urgent', 'important'):
                if image_urls:
                    task_info_list = await sync_to_async(extract_task)(full_text, image_urls=image_urls, sender_name=sender_nick)
                else:
                    task_info_list = await sync_to_async(extract_task)(full_text, sender_name=sender_nick)

                # å¦‚æœæå–å‡ºçš„æ˜¯å•ä¸ªå­—å…¸ï¼Œè½¬æˆåˆ—è¡¨ç»Ÿä¸€å¤„ç†
                if isinstance(task_info_list, dict):
                    task_info_list = [task_info_list]

                # å¦‚æœ AI è®¤ä¸ºæ²¡æœ‰ä¸å½“å‰ç”¨æˆ·ç›¸å…³çš„ä»»åŠ¡ï¼Œè¿”å›äº†ç©ºåˆ—è¡¨
                if not task_info_list:
                    reply_lines = ["âœ… å·²æ”¶åˆ°æ¶ˆæ¯ï¼Œä½†æœªè¯†åˆ«åˆ°éœ€è¦æ‚¨å¤„ç†çš„å…·ä½“ä»»åŠ¡ã€‚"]
                else:
                    reply_lines = [f"âœ… å·²ä¸ºæ‚¨è®°å½• {len(task_info_list)} ä¸ªä»»åŠ¡:"]

                    for idx, task_info in enumerate(task_info_list, 1):
                        task = await sync_to_async(Task.objects.create)(
                            title=task_info.get('title') or clean_text[:100],
                            description=task_info.get('description') or '',
                            priority=1 if classification == 'urgent' else task_info.get('priority', 2),
                            task_type=task_info.get('task_type', 'å…¶ä»–'),
                            source='dingtalk',
                            source_message_id=str(message.id),
                            due_date=task_info.get('due_date'),
                        )

                        priority_display = await sync_to_async(task.get_priority_display)()
                        task_reply = f"{idx}. {task.title} (æ‰§è¡Œäºº: {sender_nick})"
                        if task.due_date:
                            task_reply += f' [æˆªæ­¢: {task.due_date.strftime("%Y-%m-%d %H:%M")}]'
                        reply_lines.append(task_reply)

                reply = "\n".join(reply_lines)
                if url_infos:
                    new_count = sum(1 for info in url_infos if not info.get("is_existing"))
                    exist_count = len(url_infos) - new_count
                    if new_count > 0 and exist_count > 0:
                        reply += f"\n\nğŸ”— é“¾æ¥å¤„ç†å®Œæ¯•ï¼ˆ{new_count} ä¸ªæ–°ä¿å­˜ï¼Œ{exist_count} ä¸ªå·²å­˜åœ¨ï¼‰ï¼š"
                    elif new_count > 0:
                        reply += f"\n\nğŸ”— åŒæ—¶å·²å°† {new_count} ä¸ªé“¾æ¥ä¿å­˜åˆ°çŸ¥è¯†åº“ï¼š"
                    else:
                        reply += f"\n\nğŸ”— çŸ¥è¯†åº“ä¸­å·²å­˜åœ¨è¯¥é“¾æ¥ï¼š"

                    for info in url_infos:
                        status_mark = "ğŸŒŸ" if not info.get("is_existing") else "ğŸ”„ å·²æ”¶å½•"
                        reply += f"\n\n- [{info['category']}] {info['title']} ({status_mark})\n\n   è¯„åˆ†ï¼š{info['rating']}\n\n   æ¦‚è¦ï¼š\n\n{info['summary']}"
            elif classification == 'normal' or classification == 'ignore':
                if url_infos:
                    new_count = sum(1 for info in url_infos if not info.get("is_existing"))
                    exist_count = len(url_infos) - new_count

                    if new_count > 0 and exist_count > 0:
                        reply = f"âœ… é“¾æ¥å¤„ç†å®Œæ¯•ï¼ˆ{new_count} ä¸ªæ–°ä¿å­˜ï¼Œ{exist_count} ä¸ªå·²å­˜åœ¨ï¼‰ï¼š"
                    elif new_count > 0:
                        reply = f"âœ… å·²å°† {new_count} ä¸ªæ–°é“¾æ¥ä¿å­˜åˆ°çŸ¥è¯†åº“ï¼š"
                    else:
                        reply = f"ğŸ’¡ çŸ¥è¯†åº“ä¸­å·²å­˜åœ¨è¯¥é“¾æ¥ï¼š"

                    for info in url_infos:
                        status_mark = "ğŸŒŸ" if not info.get("is_existing") else "ğŸ”„ å·²æ”¶å½•"
                        reply += f"\n\n [{info['category']}] {info['title']} ({status_mark})\n\n   è¯„åˆ†ï¼š{info['rating']}\n\n   æ¦‚è¦ï¼š\n\n{info['summary']}"
                elif not is_group and classification == 'normal':
                    # åªæœ‰åœ¨å•èŠä¸”æ²¡æœ‰æå–åˆ°é“¾æ¥æ—¶ï¼Œæ‰å¯¹æ™®é€šæ¶ˆæ¯è¿›è¡Œå›å¤
                    reply = await sync_to_async(generate_reply)(text)
                else:
                    reply = None
            else:
                reply = None

            if reply:
                self.reply_text(reply, callback)
                await sync_to_async(Message.objects.create)(
                    channel_user=channel_user,
                    platform='dingtalk',
                    content=reply,
                    message_type='text',
                    direction='outbound',
                )

        except Exception:
            logger.exception("å¤„ç†æ¶ˆæ¯å¤±è´¥")
            try:
                self.reply_text('æŠ±æ­‰ï¼Œå¤„ç†æ¶ˆæ¯æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚', callback)
            except Exception:
                pass

        return AckMessage.STATUS_OK, 'OK'

    def reply_text(self, text: str, callback):
        # å…¼å®¹ SDK å·®å¼‚ï¼šæœ‰äº›ç‰ˆæœ¬çš„ callback.data å°±æ˜¯åŒ…å« senderStaffId çš„å­—å…¸ï¼Œä½†åœ¨æŸäº› SDK çš„å®ç°é‡Œ
        # ChatbotHandler éœ€è¦å°† callback è½¬å‹æˆ–è‡ªå·±å‘èµ· HTTP è¯·æ±‚ã€‚è¿™é‡Œæˆ‘ä»¬å®‰å…¨åœ°ç»™å®ƒåŒ…ä¸€å±‚å¯¹è±¡ã€‚
        if not hasattr(callback, 'sender_staff_id'):
            callback.sender_staff_id = callback.data.get('senderStaffId', '')
            callback.session_webhook = callback.data.get('sessionWebhook', '')

        self.reply_markdown('å›å¤', text, callback)
